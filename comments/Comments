
WATCH YOUR BACK, PART B - COMMENTS


"""
Project Plan Submission for Watch Your Back (part B)
COMP30024 Artificial Intelligence 2018 Semester 1

Marie-Laurence Godinot <mgodinot@student.unimelb.edu.au>
Jason Sean Pursey <jpursey@student.unimelb.edu.au>
"""



------------------------------
 GUIDE OF OUR DIFFERENT FILES:
------------------------------


Different players:
- randomplayer.py : player that takes random moves
- humanplayer.py : player that follows the instruction given by a human
- abplayer.py : player that uses minimax and alpha-beta pruning
- tdplayer.py : player that uses machine learning



--------------------------------------
 DESCRIPTION OF OUR MODULES / CLASSES:
--------------------------------------


# WATCHYOURBACK

We wanted to use the AIMA code for our minimax player: to do so, we have had to
implement a class entitled "Watchyourback" that would fit the template class of 
the AIMA code.

We have had to implement additional methods in the class "Watchyourback":
- _init_(self)
- compute_utility(board, turns_taken).





---------------------------------
REPRESENTING THE SHRINKING BOARD:
---------------------------------

In order not to get confused by the number of the rows and columns after the 
shrinking of the board, we have decided to use hyphens to represent the columns 
and the rows which no longer exist. 
Consequently, the number of the columns and the rows keep their original names 
without having to care about it. 
We have created two functions for the two shrinking cases. These functions:
1. Update the edges
2. Update the corners
3. Check if pieces are being eliminated because of the post-shrinking 
configuration.





----------------------------
OUR APPROACH OF THE PROBLEM:
----------------------------

STEP 1:

Our first step was to implement a player that just takes random legal actions. 
We tested this player by having him play against himself and it worked. We have 
then decided to head on to step 2 in order to create an intelligent agent. 




STEP 2:

After having implemented a basic "non-intelligent" player, we wanted to create a
more intelligent agent. This meant using one of the search techniques discussed
in lectures. 
We have decided to focus on Minimax. Before doing this, we knew that this 
algorithm would get tricky because of the great number of game states (it needs 
to search all the leaves of the tree, which increase exponentially to its depth)
We knew that we would have to focus on alpha-beta pruning at some point.

We have used the online supporting material for topics covered in Chapter 5 - 
Adversarial Search in the book Artificial Intelligence: A Modern Approach. 

To use the alpha-beta AIMA player, we needed to create a Watchyourback class 
that would respect the frame required by the class "Game" of their module. 
After having done this, we have implemented our minimax player with the 
following utility function: returning 1 for a winning state for white, -1 for a
winning state for black and returning 0 in other cases. 


However, when testing it, we would only get the following message:
"maximum recursion depth reached". As we had presumed, the search space was too 
big for our algorithm to work properly. 




STEP 3:

As Minimax alone wouldn't work because of its enormous search space, we have 
then decided to deal with mimimax combined with alpha-beta pruning.

This time, our player worked when testing it. 




STEP 4:

The idea was to test our alpha-beta algorithm, and to choose the value of the 
variables so as to come up with the most intelligent agent possible. 

We have had our alpha-beta algorithm play against itself, changing the depth. 


1. Equal depth between the players:

* For d = 2 for each player: 
It takes approximately 22sec and 5.5 Mb to play the entire game. 
(calculation done for 5 different games)
    
* For d = 3 for each player:
It takes approximately 1min 43sec and 5.5 Mb to play the entire game. 
(calculation done for 5 different games)


As soon as the depth increases, the amount of time taken by our player to place
or to move a piece skyrockets. To build the most intelligent agent possible, we
need to look for an agent that has a great depth but that still meets our time 
and memory contraints. 
The tricky part here is to reach a compromise between having an intelligent or a 
quick player. 


2. Different depth between the players:

The idea here was to test whether the depth was really meaningful. 
We thought that there were 2 things that ought to be considered when guessing 
who would be the winner of the game:
1. Who is starting the game ? The one who makes the first move has an advantage 
because the other player would always react (adopting a defense-like behaviour). 
In our implementation, the white was always the one who started. 
2. What is the depth of each player ? When we increase the depth, we increase 
the number of moves our player can see ahead, and we consequently have our 
player become more "intelligent". When increasing the depth of one player, we 
should consequently increase his chances of winnning the game.
That is what we have tried to check by having two player with different depths 
play against each other. 

Game  |  depths  | Winner | Moves
      |  b   w   |        | 
1        2   3      w       147
2        2   3      w       144
3        3   2      w       191
4        3   2      b       191
5        3   2      b       151
6        3   1      b       192
7        3   1      b       191

We see that if a player has a greater depth, then he is more likely to win, even 
if he is not the one who starts playing the game. 




STEP 5:

After having implemented an intelligent agent, we have decided to come up with a 
basically more intelligent and creative agent. To do so, we have try to used 
machine learning. 





-----------------------
TESTING OUR ALGORITHMS:
-----------------------

In general, we have come up with two ways of testing our algorithm:

1. we have had our algorithm play against itself;

2. and we have created a human player (humanplayer.py) so that we can play 
ourselves against our implemented players.


